---
alwaysApply: false
---

You are an expert Software Architect and Engineer.

Application techonologies :
Elixir
Phoenix
Ash : https://hexdocs.pm/ash/readme.html
WarpStream : https://docs.warpstream.com/warpstream
Broadway : https://hexdocs.pm/broadway/Broadway.html
Tigris : https://www.tigrisdata.com/docs/

Tigris instead of S3

LiveView for UI

## Test Driven Development and Testing Guidelines

### TDD Philosophy

**Primary Approach: Test Driven Development (TDD)**
- **ALWAYS prefer TDD** when requirements are clear and testable
- Follow Red-Green-Refactor cycle: Write failing test → Implement feature → Refactor
- TDD ensures code is testable, well-designed, and meets requirements from the start

**When TDD is Not Possible:**
- When requirements are unclear or exploratory
- When implementing complex integrations that need exploration first
- When prototyping or proof-of-concept work
- **In these cases**: Implement the feature first, then write comprehensive tests to achieve **100% code coverage**

### Testing Requirements from Domain Model

All features must be tested according to requirements defined in `docs/domains.md`:
- **Functional Requirements (FRs)**: Each FR must have corresponding tests
- **Acceptance Criteria**: All acceptance criteria must be verified by tests
- **Non-Functional Requirements (NFRs)**: Performance, scalability, and reliability requirements must be tested
- **Integration Points**: Cross-domain integrations must be tested
- **User Stories**: Test user flows and scenarios

### Test Coverage Requirements

**Minimum Coverage Standards:**
- **100% code coverage** for all domain logic and context modules
- **100% code coverage** for all LiveView event handlers and critical paths
- **100% code coverage** for all authentication and authorization logic
- **100% code coverage** for all domain boundary enforcement (user isolation)
- **Minimum 90% coverage** for controllers and views (UI can have lower coverage)
- **100% coverage** for all error handling and edge cases

**Coverage Exclusions:**
- Generated code (migrations, generated files)
- Configuration files
- Test helpers and fixtures
- Simple pass-through functions

### Test Structure and Organization

**Test File Organization:**
- Group related tests using `describe` blocks
- Use descriptive test names that explain what is being tested
- Follow Arrange-Act-Assert pattern

**Test Categories:**
1. **Unit Tests**: Test individual functions and modules in isolation
2. **Integration Tests**: Test interactions between modules and domains
3. **Domain Boundary Tests**: Verify user isolation and domain boundaries
4. **Event Tests**: Test event publishing, consumption, and outbox pattern
5. **LiveView Tests**: Test user interactions, real-time updates, and UI flows
6. **Controller Tests**: Test HTTP endpoints and API responses

### TDD Workflow

**When Using TDD:**

1. **Write Failing Test First**
   - Write test that describes desired behavior
   - Test should fail for the right reason (not compile errors)
   - Test should be minimal and focused

2. **Implement Minimum Code to Pass**
   - Write only enough code to make test pass
   - Don't over-engineer at this stage
   - Focus on making test green

3. **Refactor**
   - Improve code quality while keeping tests green
   - Extract functions, improve naming, reduce duplication
   - Ensure all tests still pass

4. **Repeat**
   - Continue cycle for next requirement
   - Build up functionality incrementally

### Test Implementation Guidelines

**Domain Logic Testing:**
- Test all context functions with various inputs
- Test success and error cases
- Test user isolation
- Test domain boundary enforcement
- Use fixtures for test data setup
- Test Outbox Pattern: verify events are written in same transaction

**LiveView Testing:**
- Test mount with and without authentication
- Test all event handlers (phx-click, phx-submit, phx-change)
- Test real-time updates and LiveView subscriptions
- Test user filtering in data access
- Test error handling and flash messages
- Use `Phoenix.LiveViewTest` helpers
- Test with multiple users to verify user isolation

**Integration Testing:**
- Test event publishing and consumption
- Test cross-domain interactions via events
- Test OTP calls between domains
- Test Outbox Pattern end-to-end
- Test event bus integration (PubSub, WarpStream)
- Verify domain boundaries are respected

**Authentication and Authorization Testing:**
- Test all authentication flows (login, logout, registration)
- Test authorization checks (user permissions)
- Test session management
- Test protected routes and redirects
- Test `current_scope` assignment in LiveViews and controllers

### Test Data and Fixtures

**Fixture Guidelines:**
- Create domain-specific fixtures in `test/support/fixtures/`
- Use unique data generators (unique emails, unique IDs)
- Create fixtures for users and domain entities
- Use `user_fixture/1` patterns
- Support both confirmed and unconfirmed users
- Create fixtures for different user roles and permissions

**Test Isolation:**
- Each test should be independent
- Use database transactions or sandbox mode
- Clean up test data after each test
- Use `async: true` when tests don't share state
- Use `setup` blocks for common test setup

### Testing User Isolation Scenarios

**CRITICAL**: All tests must verify user isolation where applicable:

**User Isolation Tests:**
- Test that user 1 cannot access user 2's personal data
- Test that user-specific data access filters correctly
- Test that writes validate user permissions
- Test user-scoped data access patterns

**Test Patterns:**
- Create multiple users in test setup
- Verify data is properly scoped in queries
- Test that unauthorized access attempts fail
- Test that authorized access succeeds

### Testing Event-Driven Architecture

**Event Testing Requirements:**
- Test Outbox Pattern: verify events written in same transaction as domain writes
- Test event publishing: verify events are published to event bus
- Test event consumption: verify events are consumed and processed
- Test projection creation: verify projections are created from events
- Test event ordering and idempotency
- Test event failure handling and retries

**Event Test Patterns:**
- Use `Phoenix.PubSub` test helpers for local event testing
- Mock event bus for integration tests
- Verify outbox events are created with correct payload
- Test event consumers update projections correctly
- Test event-driven domain interactions

### Performance and Load Testing

**When Required:**
- For NFRs that specify performance targets
- For high-throughput systems (event ingestion, API endpoints)
- For scalability requirements
- For critical user-facing features

**Performance Test Approach:**
- Use `Benchee` for benchmarking
- Test with realistic data volumes
- Test concurrent operations
- Verify performance targets from NFRs are met

### Test Quality Standards

**Test Code Quality:**
- Tests should be readable and self-documenting
- Use descriptive test names: `test "creates entity with valid attributes"`
- Keep tests focused: one assertion per test when possible
- Use helper functions to reduce duplication
- Follow DRY principle but prioritize clarity

**Test Maintenance:**
- Update tests when requirements change
- Refactor tests when they become hard to maintain
- Remove obsolete tests
- Keep test suite fast (use async when possible)

### Continuous Testing

**Test Execution:**
- Run tests before committing: `mix test`
- Run specific test files: `mix test test/domain/context_test.exs`
- Run tests with coverage: `mix test --cover`
- Fix failing tests immediately
- Never commit code with failing tests

**Pre-commit Checks:**
- Use `mix precommit` alias to run tests and checks
- Ensure all tests pass before pushing
- Verify code coverage meets requirements
- Fix any linting or formatting issues

### Testing Checklist

When implementing features, ensure:
- [ ] TDD used when possible, or 100% coverage achieved if TDD not used
- [ ] All functional requirements (FRs) have tests
- [ ] All acceptance criteria are tested
- [ ] User isolation is tested (when applicable)
- [ ] Domain boundaries are tested
- [ ] Event publishing/consumption is tested
- [ ] Outbox Pattern is tested
- [ ] Error cases are tested
- [ ] Edge cases are tested
- [ ] Integration points are tested
- [ ] Code coverage meets minimum requirements
- [ ] Tests are readable and maintainable
- [ ] Tests run in CI/CD pipeline 

